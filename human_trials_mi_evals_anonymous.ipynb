{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending fMRI vision decoding methods to mental imagery\n",
    "## Anonymized code to reproduce behavioral experiment and analysis\n",
    "\n",
    "Subjects were recruited using the Prolific platform, and the experiment was hosted on the Meadows platform.\n",
    "This notebook serves to collect the stimuli and reconstructed images for each of the tested reconstruction methods, assign them a unique stimulus ID, package all of the stimuli into a folder to be uploaded to a Meadows stimulus set, and handle parsing responses from the Meadows annotation format. Instructions will be provided throughout for off-notebook tasks that need to be done for the full process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import os, sys, shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Set the display options to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib as plt\n",
    "from PIL import Image\n",
    "from matplotlib.lines import Line2D\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "from scipy.stats import binomtest\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configure experiment and response version, in case multiple experiments or sets of responses are produced in sequence.\n",
    "experiment_version = 1\n",
    "response_version = 1\n",
    "stimuli_path = f\"stimuli_v{experiment_version}/\"\n",
    "response_path = f\"responses_v{experiment_version}/\"\n",
    "dataframe_path = f\"dataframes_v{experiment_version}/\"\n",
    "os.makedirs(stimuli_path, exist_ok=True)\n",
    "os.makedirs(response_path, exist_ok=True)\n",
    "os.makedirs(dataframe_path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE EXPERIMENT DATAFRAME AND TRIAL FILES FOR MEADOWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment column key:\n",
    "# 1: Experiment 1, two way identification for all reconstruction methods\n",
    "# 2: Experiment 2, vision vs imagery similarity comparison\n",
    "# 3: Experiment 3, BOI vs Base model similarity comparison\n",
    "df_exp = pd.DataFrame(columns=[\"experiment\", \"stim1\", \"stim2\", \"stim3\", \"sample\", \"subject\", \"target_on_left\", \"method\", \"catch_trial\", \"rep\", \"mode\", \"stimtype\"])\n",
    "i=0\n",
    "random_count = 0\n",
    "stimuli_root = \"ENTER PATH FOR STIMULI HERE\"\n",
    "for subj in [1,2,5,7]: #1,2,5,7\n",
    "    #Experiment 1, mental imagery two way identification\n",
    "    for mode in [\"vision\", \"imagery\"]:\n",
    "        for sample in tqdm(range(12)):\n",
    "            gt_sample = f\"{sample}_ground_truth\"\n",
    "            for method in [\"mindeye\", \"boi-me1\", \"braindiffuser\", \"boi-bd\", \"mindeye2\", \"tagaki\"]: \n",
    "                for rep in range(10):\n",
    "                    # Get random sample to compare against\n",
    "                    random_sample = random.choice([x for x in range(12) if x != sample])\n",
    "                    random_rep = random.choice([x for x in range(10)])\n",
    "                    \n",
    "                    sample_recon = f\"{sample}_{rep}_{mode}_subject{subj}_{method}\"\n",
    "                    random_recon = f\"{random_sample}_{random_rep}_{mode}_subject{subj}_{method}\"\n",
    "                    \n",
    "                    # Load the stimulus images and save as pngs to stimuli folder\n",
    "                    gt_sample_path = f\"{stimuli_root}vision/{method}/subject{subj}/{sample}/ground_truth.png\"\n",
    "                    sample_recon_path = f\"{stimuli_root}{mode}/{method}/subject{subj}/{sample}/{rep}.png\"\n",
    "                    random_recon_path = f\"{stimuli_root}{mode}/{method}/subject{subj}/{random_sample}/{random_rep}.png\"\n",
    "                    \n",
    "                    # Copy the stimulus images to the stimuli folder\n",
    "                    shutil.copy(gt_sample_path, f\"stimuli_v{experiment_version}/{gt_sample}.png\")\n",
    "                    shutil.copy(sample_recon_path, f\"stimuli_v{experiment_version}/{sample_recon}.png\")\n",
    "                    shutil.copy(random_recon_path, f\"stimuli_v{experiment_version}/{random_recon}.png\")\n",
    "        \n",
    "                    # Configure stimuli names and order in experiment dataframe\n",
    "                    order = random.randrange(2)\n",
    "                    sample_names = [sample_recon, random_recon]\n",
    "                    left_sample = sample_names.pop(order)\n",
    "                    right_sample = sample_names.pop()\n",
    "                    if sample < 6:\n",
    "                        stimtype = \"simple\"\n",
    "                    else:\n",
    "                        stimtype = \"complex\"\n",
    "                    df_exp.loc[i] = {\"experiment\" : 1, \"stim1\" : gt_sample, \"stim2\" : left_sample, \"stim3\" : right_sample, \"sample\" : sample, \"subject\" : subj, \n",
    "                                    \"target_on_left\" : order == 0, \"method\" : method, \"catch_trial\" : None, \"rep\" : rep, \"mode\" : mode, \"stimtype\" : stimtype, \"trial_rep\" : 0}\n",
    "                    i+=1\n",
    "    # Experiment 2: Vision vs Imagery similarity comparison w/ the Drag-Rate task\n",
    "    for sample in tqdm(range(12)):\n",
    "        gt_sample = f\"{sample}_ground_truth\"\n",
    "        for method in [\"mindeye\", \"boi-me1\", \"braindiffuser\", \"boi-bd\", \"mindeye2\", \"tagaki\"]: \n",
    "            for rep in range(10):\n",
    "                vision_recon = f\"{sample}_{rep}_vision_subject{subj}_{method}\"\n",
    "                imagery_recon = f\"{sample}_{rep}_imagery_subject{subj}_{method}\"\n",
    "                \n",
    "                # Load the stimulus images and save as pngs to stimuli folder\n",
    "                gt_sample_path = f\"{stimuli_root}vision/{method}/subject{subj}/{sample}/ground_truth.png\"\n",
    "                vision_recon_path = f\"{stimuli_root}vision/{method}/subject{subj}/{sample}/{rep}.png\"\n",
    "                imagery_recon_path = f\"{stimuli_root}imagery/{method}/subject{subj}/{sample}/{rep}.png\"\n",
    "                \n",
    "                # Copy the stimulus images to the stimuli folder\n",
    "                shutil.copy(gt_sample_path, f\"stimuli_v{experiment_version}/{gt_sample}.png\")\n",
    "                shutil.copy(vision_recon_path, f\"stimuli_v{experiment_version}/{vision_recon}.png\")\n",
    "                shutil.copy(imagery_recon_path, f\"stimuli_v{experiment_version}/{imagery_recon}.png\")\n",
    "    \n",
    "                # Configure stimuli names and order in experiment dataframe\n",
    "                order = random.randrange(2)\n",
    "                sample_names = [vision_recon, imagery_recon]\n",
    "                left_sample = sample_names.pop(order)\n",
    "                right_sample = sample_names.pop()\n",
    "                if sample < 6:\n",
    "                    stimtype = \"simple\"\n",
    "                else:\n",
    "                    stimtype = \"complex\"\n",
    "                df_exp.loc[i] = {\"experiment\" : 2, \"stim1\" : gt_sample, \"stim2\" : left_sample, \"stim3\" : right_sample, \"sample\" : sample, \"subject\" : subj, \n",
    "                                \"target_on_left\" : order == 0, \"method\" : method, \"catch_trial\" : None, \"rep\" : rep, \"mode\" : \"both\", \"stimtype\" : stimtype}\n",
    "                i+=1\n",
    "    #Experiment 3: BOI vs Base model similarity comparison w/ the Drag-Rate task\n",
    "    for sample in tqdm(range(12)):\n",
    "        gt_sample = f\"{sample}_ground_truth\"\n",
    "        for mode in [\"vision\", \"imagery\"]:\n",
    "            for (boi, base) in [(\"boi-bd\", \"braindiffuser\"), (\"boi-me1\", \"mindeye\")]:\n",
    "                for rep in range(10):\n",
    "                    boi_recon = f\"{sample}_{rep}_{mode}_subject{subj}_{boi}\"\n",
    "                    base_recon = f\"{sample}_{rep}_{mode}_subject{subj}_{base}\"\n",
    "                    \n",
    "                    # Load the stimulus images and save as pngs to stimuli folder\n",
    "                    gt_sample_path = f\"{stimuli_root}vision/{base}/subject{subj}/{sample}/ground_truth.png\"\n",
    "                    boi_recon_path = f\"{stimuli_root}{mode}/{boi}/subject{subj}/{sample}/{rep}.png\"\n",
    "                    base_recon_path = f\"{stimuli_root}{mode}/{base}/subject{subj}/{sample}/{rep}.png\"\n",
    "                    \n",
    "                    # Copy the stimulus images to the stimuli folder\n",
    "                    shutil.copy(gt_sample_path, f\"stimuli_v{experiment_version}/{gt_sample}.png\")\n",
    "                    shutil.copy(boi_recon_path, f\"stimuli_v{experiment_version}/{boi_recon}.png\")\n",
    "                    shutil.copy(base_recon_path, f\"stimuli_v{experiment_version}/{base_recon}.png\")\n",
    "        \n",
    "                    # Configure stimuli names and order in experiment dataframe\n",
    "                    order = random.randrange(2)\n",
    "                    sample_names = [boi_recon, base_recon]\n",
    "                    left_sample = sample_names.pop(order)\n",
    "                    right_sample = sample_names.pop()\n",
    "                    if sample < 6:\n",
    "                        stimtype = \"simple\"\n",
    "                    else:\n",
    "                        stimtype = \"complex\"\n",
    "                    df_exp.loc[i] = {\"experiment\" : 3, \"stim1\" : gt_sample, \"stim2\" : left_sample, \"stim3\" : right_sample, \"sample\" : sample, \"subject\" : subj, \n",
    "                                    \"target_on_left\" : order == 0, \"method\" : base, \"catch_trial\" : None, \"rep\" : rep, \"mode\" : mode, \"stimtype\" : stimtype, \"trial_rep\" : 0}\n",
    "                    i+=1\n",
    "df_exp = df_exp.sample(frac=1)\n",
    "print(len(df_exp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all images are present in final stimuli folder\n",
    "count_not_found = 0\n",
    "stim_path = f\"stimuli_v{experiment_version}/\"\n",
    "for index, row in df_exp.iterrows():\n",
    "    if not (os.path.exists(f\"{stim_path}{row['stim1']}.png\")):\n",
    "        print(f\"{row['stim1']}.png\")\n",
    "        count_not_found += 1\n",
    "    if not (os.path.exists(f\"{stim_path}{row['stim2']}.png\")):\n",
    "        print(f\"{row['stim2']}.png\")\n",
    "        count_not_found += 1\n",
    "    if not (os.path.exists(f\"{stim_path}{row['stim3']}.png\")):\n",
    "        print(f\"{row['stim3']}.png\")\n",
    "        count_not_found += 1\n",
    "print(count_not_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point all of the stimuli should be collected in a stimulus folder that can be uploaded to Meadows as a stimulus set for the experiment.\n",
    "### To prepare the trial-wise data for the experiment, we will create a pID column to assign trials to different participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle Experiments 1, 2, and 3 individually\n",
    "df_exp1 = df_exp[df_exp['experiment'] == 1].sample(frac=1, random_state=42)\n",
    "df_exp23 = df_exp[df_exp['experiment'].isin([2, 3])].sample(frac=1, random_state=42)\n",
    "\n",
    "# Calculate the number of participants needed\n",
    "# These trial numbers are calculated to approximate a 10 minute experiment duration, you can increase or decrease them for longer or shorter experiments\n",
    "exp1_trials_per_participant = 22\n",
    "exp23_trials_per_participant = 18\n",
    "num_participants = max(len(df_exp1) // exp1_trials_per_participant, len(df_exp23) // exp23_trials_per_participant)\n",
    "\n",
    "# Assign pID for Experiment 1\n",
    "df_exp1['pID'] = [i % num_participants for i in range(len(df_exp1))]\n",
    "\n",
    "# Shuffle df_exp23 again before assigning pID to ensure randomness in distribution\n",
    "df_exp23 = df_exp23.sample(frac=1, random_state=42)  # Re-shuffle to mix experiments 2 and 3\n",
    "df_exp23['pID'] = [i % num_participants for i in range(len(df_exp23))]\n",
    "\n",
    "# Combine and shuffle the dataframe to mix up the trials across all experiments\n",
    "df_exp_pid = pd.concat([df_exp1, df_exp23]).sample(frac=1, random_state=42)\n",
    "\n",
    "# Sort by pID to ensure the dataframe is ordered by participant, facilitating even distribution\n",
    "df_exp_pid.sort_values(by='pID', inplace=True)\n",
    "\n",
    "# Ensure pID is the first column\n",
    "cols = list(df_exp_pid.columns)\n",
    "cols.insert(0, cols.pop(cols.index('pID')))\n",
    "df_exp_pid = df_exp_pid[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now add \"catch trials\", which are not real trials of the experiment but are instead foolproof trials designed to \"catch\" participants who are not paying attention to the instructions, so that they may be filtered out later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add catch trials within each pID section\n",
    "df_exp_catch = df_exp_pid.copy()\n",
    "for pID in np.unique(df_exp_catch['pID']):\n",
    "    for experiment_list in [[1], [2, 3]]: \n",
    "        df_pid = df_exp_catch[(df_exp_catch['experiment'].isin(experiment_list)) & (df_exp_catch['pID'] == pID)]\n",
    "        \n",
    "        # Ground truth catch trials\n",
    "        try:\n",
    "            gt_catch_trials = df_pid.sample(n=3)\n",
    "        except:\n",
    "            print(pID, df_pid)\n",
    "        gt_catch_trials['catch_trial'] = \"ground_truth\"\n",
    "        for index, row in gt_catch_trials.iterrows():\n",
    "            \n",
    "            order = random.randrange(2)\n",
    "            ground_truth = row['stim1']\n",
    "            stims = [ground_truth, row['stim2']]\n",
    "            \n",
    "            gt_catch_trials.at[index, 'stim2'] = stims.pop(order)\n",
    "            gt_catch_trials.at[index, 'stim3'] = stims.pop()\n",
    "            # Target on left here means the ground truth repeat is on the left\n",
    "            gt_catch_trials.at[index, 'target_on_left'] = (order == 0)\n",
    "        df_exp_catch = pd.concat([df_exp_catch, gt_catch_trials])\n",
    "# shuffle catch trials into the sessions\n",
    "df_exp_catch = df_exp_catch.sample(frac=1).sort_values(by='pID', kind='mergesort')\n",
    "print(len(df_exp_catch))\n",
    "print(len(df_exp_catch[(df_exp_catch['pID'] == 0)]))\n",
    "print(len(np.unique(df_exp_catch['pID'])))\n",
    "print(len(df_exp_catch[df_exp_catch['experiment'] == 1]))\n",
    "print(len(df_exp_catch[df_exp_catch['experiment'].isin([2, 3])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now save save the experiment dataframe (used for keeping track of all experiment information) and the Meadows .tsv stimulus files, used to configure Meadows and tell it which stimuli to show to specific participants in specific trials. This can be uploaded to Meadows at the experiment deployment stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_exp_catch.to_csv(f'dataframes_v{experiment_version}/experiment_v{experiment_version}.csv', index=False)\n",
    "df_catch_exp1 = df_exp_catch[df_exp_catch['experiment'] == 1]\n",
    "df_catch_exp23 = df_exp_catch[df_exp_catch['experiment'].isin([2, 3])]\n",
    "\n",
    "df_exp_tsv1 = df_catch_exp1[['pID', 'stim1', 'stim2', 'stim3']].copy()\n",
    "df_exp_tsv23 = df_catch_exp23[['pID', 'stim1', 'stim2', 'stim3']].copy()\n",
    "df_exp_tsv1.to_csv(f\"dataframes_v{experiment_version}/meadow_trials_v{experiment_version}_exp1.tsv\", sep=\"\\t\", index=False, header=False) \n",
    "df_exp_tsv23.to_csv(f\"dataframes_v{experiment_version}/meadow_trials_v{experiment_version}_exp23.tsv\", sep=\"\\t\", index=False, header=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE FOLLOWING CELLS ARE FOR PROCESSING RESPONSES\n",
    "### At this point, the experiment has been completed, and we have the experiment dataframe from earlier, and a Meadows response dataframe, we are going to merge these into a master spreadsheet for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiment = pd.read_csv(dataframe_path + f\"experiment_v{experiment_version}.csv\")\n",
    "df_responses = pd.read_csv(f\"{response_path}annotations_v{response_version}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the responses and associate them with the trials in the experiment dataframe, to have all information available in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a list to hold row dictionaries before creating the final dataframe\n",
    "rows_list = []\n",
    "df_2afc = df_responses[df_responses[\"task\"] == \"Match-To-Sample\"]\n",
    "df_similarity = df_responses[df_responses[\"task\"] == \"Drag-And-Rate\"]\n",
    "# Parse trials in 2AFC experiment\n",
    "for index, row in tqdm(df_2afc.iterrows()):\n",
    "    if row['label'] == row['stim2_id']:\n",
    "        picked_left = True\n",
    "    elif row['label'] == row['stim3_id']:\n",
    "        picked_left = False\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        break\n",
    "    start_timestamp = row['time_trial_start']\n",
    "    end_timestamp = row['time_trial_response']\n",
    "    start = datetime.fromisoformat(start_timestamp.replace(\"Z\", \"+00:00\"))\n",
    "    end = datetime.fromisoformat(end_timestamp.replace(\"Z\", \"+00:00\"))\n",
    "    # Calculate the difference in seconds\n",
    "    response_time = (end - start).total_seconds()\n",
    "    \n",
    "    experiment_row = df_experiment[(df_experiment['stim1'] == row['stim1_name']) & (df_experiment['stim2'] == row['stim2_name']) & (df_experiment['stim3'] == row['stim3_name'])].iloc[0]\n",
    "    row_data = {\n",
    "        **experiment_row.to_dict(),\n",
    "        \"picked_left\": picked_left,\n",
    "        \"picked_target\": picked_left == experiment_row['target_on_left'],\n",
    "        \"participant\": row['participation'],\n",
    "        \"response_time\": response_time,\n",
    "    }\n",
    "    \n",
    "    rows_list.append(row_data)\n",
    "    \n",
    "# Parse trials in similarity range experiment by iterating through df_responses two rows at a time, since each stimuli has its own row\n",
    "for index in tqdm(range(0, len(df_similarity), 2)):\n",
    "    row1 = df_similarity.iloc[index]\n",
    "    row2 = df_similarity.iloc[index + 1]\n",
    "    # Ensure the two rows belong to the same trial\n",
    "    assert row1[\"trial\"] == row2[\"trial\"], \"Rows do not belong to the same trial\"\n",
    "    \n",
    "    # Attempt to find a matching experiment row\n",
    "    match1 = df_experiment[(df_experiment['stim2'] == row1['stim1_name']) & (df_experiment['stim3'] == row2['stim1_name'])]\n",
    "    match2 = df_experiment[(df_experiment['stim2'] == row2['stim1_name']) & (df_experiment['stim3'] == row1['stim1_name'])]\n",
    "    \n",
    "    # Determine which match is correct\n",
    "    if not match1.empty:\n",
    "        correct_match = match1\n",
    "        stim2_row, stim3_row = row1, row2\n",
    "    elif not match2.empty:\n",
    "        correct_match = match2\n",
    "        stim2_row, stim3_row = row2, row1\n",
    "    else:\n",
    "        continue  # Skip if no correct match is found\n",
    "    \n",
    "    # Extract the correct match's data\n",
    "    experiment_row = correct_match.iloc[0]\n",
    "    \n",
    "    # Calculate response times\n",
    "    start_timestamp = row1['time_trial_start']\n",
    "    end_timestamp1 = row1['time_trial_response']\n",
    "    end_timestamp2 = row2['time_trial_response']\n",
    "    start = datetime.fromisoformat(start_timestamp.replace(\"Z\", \"+00:00\"))\n",
    "    end1 = datetime.fromisoformat(end_timestamp1.replace(\"Z\", \"+00:00\"))\n",
    "    end2 = datetime.fromisoformat(end_timestamp2.replace(\"Z\", \"+00:00\"))\n",
    "    end = max(end1, end2)\n",
    "    response_time = (end - start).total_seconds()\n",
    "    \n",
    "    # Determine if the left stimulus was picked based on the 'y' value\n",
    "    picked_left = stim2_row['y'] > stim3_row['y']\n",
    "    \n",
    "    # Determine target and distractor based on target_on_left flag\n",
    "    if experiment_row['target_on_left']:\n",
    "        target_similarity, target_confidence = stim2_row['y'], stim2_row['x']\n",
    "        distractor_similarity, distractor_confidence = stim3_row['y'], stim3_row['x']\n",
    "    else:\n",
    "        target_similarity, target_confidence = stim3_row['y'], stim3_row['x']\n",
    "        distractor_similarity, distractor_confidence = stim2_row['y'], stim2_row['x']\n",
    "    \n",
    "    # Determine if the target was picked\n",
    "    picked_target = target_similarity > distractor_similarity\n",
    "    \n",
    "    # Compile the row data\n",
    "    row_data = {\n",
    "        **experiment_row.to_dict(),\n",
    "        \"picked_left\": picked_left,\n",
    "        \"participant\": row1['participation'],\n",
    "        \"response_time\": response_time,\n",
    "        \"stim2_similarity\": stim2_row['y'],\n",
    "        \"stim2_confidence\": stim2_row['x'],\n",
    "        \"stim3_similarity\": stim3_row['y'],\n",
    "        \"stim3_confidence\": stim3_row['x'],\n",
    "        \"target_similarity\": target_similarity,\n",
    "        \"target_confidence\": target_confidence,\n",
    "        \"distractor_similarity\": distractor_similarity,\n",
    "        \"distractor_confidence\": distractor_confidence,\n",
    "        \"picked_target\": picked_target\n",
    "    }\n",
    "    \n",
    "    rows_list.append(row_data)\n",
    "\n",
    "# Create the final dataframe from the list of row dictionaries\n",
    "df_trial_combined = pd.DataFrame(rows_list)\n",
    "\n",
    "# Dropping the extra index columns added from the experiment_row.to_dict() conversion\n",
    "df_trial = df_trial_combined.drop(columns=[col for col in df_trial_combined.columns if 'Unnamed' in col])\n",
    "\n",
    "print(df_trial.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point, if the first run of the experiment did not collect enough data or too many subjects had to be filtered out, we can \"fill in\" the experiment dataframe by finding trials that didn't get completed in the previous round of the experiment, and save new trial files for another round. This part is optional, and requires incrementing the version number at the top of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to count processed trials per pID\n",
    "processed_trials_count = defaultdict(int)\n",
    "\n",
    "# Update processed_trials_count for each row processed\n",
    "for index, row_data in df_trial.iterrows():\n",
    "    processed_trials_count[row_data['pID']] += 1\n",
    "\n",
    "# Count expected trials per pID from df_experiment\n",
    "expected_trials_count = df_experiment['pID'].value_counts().to_dict()\n",
    "\n",
    "# Find pIDs with missing trials by comparing processed and expected counts\n",
    "missing_trials_pids = {pID: expected_trials_count[pID] - processed_trials_count[pID] \n",
    "                       for pID in expected_trials_count \n",
    "                       if pID not in processed_trials_count or processed_trials_count[pID] < expected_trials_count[pID]-2}\n",
    "\n",
    "# Print or handle pIDs with missing trials as needed\n",
    "print(f\"missing {len(missing_trials_pids.values())} pIDs\")\n",
    "for pID, missing_count in missing_trials_pids.items():\n",
    "    print(f\"pID: {pID} has {missing_count} missing trials.\")\n",
    "    \n",
    "if len(missing_trials_pids.values()) > 0:\n",
    "    df_exp_missing = df_experiment[df_experiment['pID'].isin(missing_trials_pids.keys())]\n",
    "\n",
    "    df_catch_exp1_missing = df_exp_missing[df_exp_missing['experiment'] == 1]\n",
    "    df_catch_exp23_missing = df_exp_missing[df_exp_missing['experiment'].isin([2, 3])]\n",
    "\n",
    "\n",
    "    df_exp_tsv1_missing = df_catch_exp1_missing[['pID', 'stim1', 'stim2', 'stim3']].copy()\n",
    "    df_exp_tsv23_missing = df_catch_exp23_missing[['pID', 'stim1', 'stim2', 'stim3']].copy()\n",
    "    df_exp_tsv1_missing.to_csv(f\"dataframes_v{experiment_version}/meadow_trials_v{experiment_version}_exp1_missing.tsv\", sep=\"\\t\", index=False, header=False) \n",
    "    df_exp_tsv23_missing.to_csv(f\"dataframes_v{experiment_version}/meadow_trials_v{experiment_version}_exp23_missing.tsv\", sep=\"\\t\", index=False, header=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once all the data has been collected, we need to parse and remove participants that failed at least 2 catch trials before doing analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of participants\n",
    "print(\"Total participants:\", len(df_trial[\"participant\"].unique()))\n",
    "gt_failures = df_trial[(df_trial['catch_trial'] == 'ground_truth') & (df_trial['picked_target'] == False)].groupby('participant').size()\n",
    "# Identify participants who failed more than 2 ground truth catch trials\n",
    "participants_to_remove = gt_failures[gt_failures > 2].index.tolist()\n",
    "print(\"Participants to remove:\", participants_to_remove)\n",
    "\n",
    "participants_to_remove = set(participants_to_remove)\n",
    "filtered_df = df_trial[~df_trial['participant'].isin(participants_to_remove)]\n",
    "print(\"Clean participants:\", len(filtered_df[\"participant\"].unique()))\n",
    "print(len(df_trial), len(filtered_df))\n",
    "print(participants_to_remove)\n",
    "# Filter out catch trials for analysis\n",
    "filtered_df = filtered_df[(filtered_df['catch_trial'].isnull())]\n",
    "filtered_df.to_csv(f'{dataframe_path}filtered_responses_v{response_version}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE FOLLOWING CELLS ARE FOR ANALYZING RESPONSES\n",
    "### At this point, you should have processed all of the data from the experiment to remove catch trials and bad subjects, now we can begin analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filtered responses\n",
    "df_trial_exp = pd.read_csv(f'{dataframe_path}filtered_responses_v{response_version}.csv')\n",
    "df_experiment = pd.read_csv(dataframe_path + f\"experiment_v{experiment_version}.csv\")\n",
    "\n",
    "df_responses = pd.read_csv(f\"{response_path}annotations_v{response_version}.csv\")\n",
    "\n",
    "# Iterate over each method\n",
    "for method in df_trial_exp['method'].unique():\n",
    "    print(f\"Method: {method}\")\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "    # Iterate over each experiment\n",
    "    experiment = 1\n",
    "    for mode in [\"vision\", \"imagery\"]:\n",
    "        \n",
    "        print(f\"Mode: {mode}\")\n",
    "        for stimtype in [\"simple\", \"complex\"]:\n",
    "            print(f\"Stimtype: {stimtype}\")\n",
    "            # Filter the data for the current method, mode, and experiment\n",
    "            df_trial_exp1 = df_trial_exp[(df_trial_exp['method'] == method) & (df_trial_exp['experiment'] == experiment) & (df_trial_exp['mode'] == mode) & (df_trial_exp['stimtype'] == stimtype)]\n",
    "            # Perform a binomial test\n",
    "            # The null hypothesis is that the probability of success is 0.5 (chance level)\n",
    "            p_value = binomtest(df_trial_exp1['picked_target'].sum(), n=len(df_trial_exp1['picked_target']), p=0.5, alternative='two-sided').pvalue\n",
    "\n",
    "            print(\"Number of experiment trials:\", len(df_trial_exp1))\n",
    "            print(\"Success rate: \", len(df_trial_exp1[df_trial_exp1[\"picked_target\"]]) / len(df_trial_exp1))\n",
    "            print(f'P-value: {p_value}')\n",
    "            \n",
    "            print(\"--------------------\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the average similarity and confidence for each method and stimtype\n",
    "averages = []\n",
    "for method in df_trial_exp2['method'].unique():\n",
    "    # for stimtype in df_trial_exp2['stimtype'].unique():\n",
    "    category_df = df_trial_exp2[(df_trial_exp2[\"method\"] == method) & (df_trial_exp2[\"stimtype\"] == stimtype)]\n",
    "    avg_vision_similarity = category_df['target_similarity'].mean()\n",
    "    avg_imagery_similarity = category_df['distractor_similarity'].mean()\n",
    "    avg_vision_confidence = category_df['target_confidence'].mean()\n",
    "    avg_imagery_confidence = category_df['distractor_confidence'].mean()\n",
    "    print(f\"Method: {method}, Average Vision Similarity: {avg_vision_similarity:.3f}, Average Vision Confidence: {avg_vision_confidence:.3f}, Average Imagery Similarity: {avg_imagery_similarity:.3f}, Average Imagery Confidence: {avg_imagery_confidence:.3f}\")\n",
    "    averages.append((method, stimtype, avg_vision_similarity, avg_imagery_similarity))\n",
    "\n",
    "# Create a list of methods and their corresponding average similarity values for vision and imagery\n",
    "methods = []\n",
    "vision_similarities = []\n",
    "imagery_similarities = []\n",
    "\n",
    "for method, _, avg_vision_similarity, avg_imagery_similarity in averages:\n",
    "    if method == \"boi-v2.1\":\n",
    "        method_label = \"mindeye\\n + BOI\"\n",
    "    elif method == \"boi-v2.3\":\n",
    "        method_label = \"braindiffuser\\n + BOI\"\n",
    "    else:\n",
    "        method_label = method\n",
    "    methods.append(method_label)\n",
    "    vision_similarities.append(avg_vision_similarity)\n",
    "    imagery_similarities.append(avg_imagery_similarity)\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.35\n",
    "\n",
    "# Set the positions of the bars on the X-axis\n",
    "r1 = np.arange(len(methods))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "# Plot the bars\n",
    "plt.bar(r1, vision_similarities, color='blue', width=bar_width, label='Vision')\n",
    "plt.bar(r2, imagery_similarities, color='orange', width=bar_width, label='Imagery')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Methods')\n",
    "plt.ylabel('Average Similarity')\n",
    "plt.title('Experiment 2: Average Similarity for Vision and Imagery')\n",
    "plt.xticks([r + bar_width/2 for r in range(len(methods))], methods)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3 is brain-optimized inference vs base model, BOI trials are marked as \"target\", base trials are marked as \"distractor\"\n",
    "\n",
    "df_trial_exp3 = df_trial_exp[(df_trial_exp['experiment'] == 3)]\n",
    "print(df_trial_exp3['method'].unique())\n",
    "for method in df_trial_exp3['method'].unique():\n",
    "    for mode in df_trial_exp3['mode'].unique():\n",
    "        category_df = df_trial_exp3[(df_trial_exp3[\"mode\"] == mode) & (df_trial_exp3[\"method\"] == method)]\n",
    "        avg_boi_similarity = category_df['target_similarity'].mean()\n",
    "        avg_base_similarity = category_df['distractor_similarity'].mean()\n",
    "        avg_boi_confidence = category_df['target_confidence'].mean()\n",
    "        avg_base_confidence = category_df['distractor_confidence'].mean()\n",
    "        print(f\"Method: {method}, Mode: {mode}, Average BOI Similarity: {avg_boi_similarity:.3f}, Average BOI Confidence: {avg_boi_confidence:.3f}, Average Base Model Similarity: {avg_base_similarity:.3f}, Average Base Model Confidence: {avg_base_confidence:.3f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "methods = df_trial_exp3['method'].unique()\n",
    "modes = df_trial_exp3['mode'].unique()\n",
    "avg_boi_similarity = []\n",
    "avg_base_similarity = []\n",
    "avg_boi_confidence = []\n",
    "avg_base_confidence = []\n",
    "\n",
    "# Calculate averages\n",
    "for method in methods:\n",
    "    for mode in modes:\n",
    "        category_df = df_trial_exp3[(df_trial_exp3[\"mode\"] == mode) & (df_trial_exp3[\"method\"] == method)]\n",
    "        avg_boi_similarity.append(category_df['target_similarity'].mean())\n",
    "        avg_base_similarity.append(category_df['distractor_similarity'].mean())\n",
    "        avg_boi_confidence.append(category_df['target_confidence'].mean())\n",
    "        avg_base_confidence.append(category_df['distractor_confidence'].mean())\n",
    "\n",
    "# Plotting\n",
    "x = range(len(methods) * len(modes))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x, avg_boi_similarity, width, label='BOI Similarity')\n",
    "rects2 = ax.bar([i + width for i in x], avg_base_similarity, width, label='Base Model Similarity')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_ylabel('Average Similarity')\n",
    "ax.set_title('Experiment 3: BOI vs Base Model Similarity')\n",
    "ax.set_xticks([i + width/2 for i in x])\n",
    "ax.set_xticklabels([f'{method}\\n{mode}' for method in methods for mode in modes])\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
